{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of nmt_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UtpalMattoo/BERT-Transformers/blob/master/Copy_of_nmt_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qNSzzyaCbD"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jmjh290raIky"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYj59gynA8mH"
      },
      "source": [
        "[Utpal 09/30/2020]: Don't run section on \"Training\" when mounting/restoring checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T04eSHVbBO5-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpGoE2T-YXS"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/nmt_with_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation. This is an advanced example that assumes some knowledge of sequence to sequence models.\n",
        "\n",
        "After training the model in this notebook, you will be able to input a Spanish sentence, such as *\"¿todavia estan en casa?\"*, and return the English translation: *\"are you still at home?\"*\n",
        "\n",
        "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n",
        "\n",
        "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
        "\n",
        "Note: This example takes approximately 10 minutes to run on a single P100 GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
        "\n",
        "```\n",
        "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
        "```\n",
        "\n",
        "There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
        "\n",
        "1. Add a *start* and *end* token to each sentence.\n",
        "2. Clean the sentences by removing special characters.\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4. Pad each sentence to a maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b",
        "outputId": "6db4068a-8155-402c-c9cd-1dba980f9ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd0jw-eC3jEh"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "# https://medium.com/concerning-pharo/an-implementation-of-unicode-normalization-7c6719068f43\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opI2GzOt479E",
        "outputId": "6eefd4ad-ad73-48c4-fdde-42e262f3ce6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "en_sentence = u\"May I borrow this book?\"\n",
        "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTbSbBz55QtF",
        "outputId": "c7af7d09-2b2e-4392-a114-f33728315a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIOn8RCNDJXG"
      },
      "source": [
        "# https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
        "\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "  print (targ_lang[0])\n",
        "  print (inp_lang[0])\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  print (input_tensor.shape)\n",
        "  print (input_tensor[0])\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "  print (target_tensor.shape)\n",
        "  print (target_tensor[0])\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n",
        "\n",
        "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnxC7q-j3jFD",
        "outputId": "0ed7d37b-9ce8-49b4-8f4f-7e34cfb99af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "print (target_tensor.shape)\n",
        "print (input_tensor.shape)\n",
        "print (max_length_inp)\n",
        "print (max_length_targ)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> go . <end>\n",
            "<start> ve . <end>\n",
            "(30000, 16)\n",
            "[  1 135   3   2   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "(30000, 11)\n",
            "[ 1 36  3  2  0  0  0  0  0  0  0]\n",
            "(30000, 11)\n",
            "(30000, 16)\n",
            "16\n",
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QILQkOs3jFG",
        "outputId": "650a13f7-2da0-42fe-f89c-8ed20b428419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print (input_tensor_train.shape)\n",
        "print (target_tensor_train.shape)\n",
        "\n",
        "# # Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24000, 16)\n",
            "(24000, 11)\n",
            "24000 24000 6000 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPmLZGMeD5q"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXukARTDd7MT",
        "outputId": "ff6d021a-143f-4b94-f982-69338258a13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[1])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> tom\n",
            "1059 ----> encendio\n",
            "9 ----> el\n",
            "4190 ----> horno\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "5 ----> tom\n",
            "874 ----> lit\n",
            "13 ----> the\n",
            "2754 ----> oven\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqHsArVZ3jFS",
        "outputId": "9e95ae05-92db-4a90-e776-454b1016406d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "print (len(input_tensor_train))\n",
        "print (steps_per_epoch)\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "print (\"vocab_inp_size = {} \".format(vocab_inp_size))\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "print (\"vocab_tar_size = {} \".format(vocab_tar_size))\n",
        "\n",
        "# https://www.tensorflow.org/guide/data#dataset_structure\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Some test code to understand class tf.data.Dataset\n",
        "\n",
        "UnderstandDatasetTakeEtc = True\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    # print (dataset.take(steps_per_epoch))\n",
        "    # print (enumerate(dataset.take(steps_per_epoch)))   \n",
        "    for (batch) in enumerate(dataset.take(steps_per_epoch)):\n",
        "       if ((epoch==0) and (UnderstandDatasetTakeEtc)): \n",
        "            print (batch) #this will print the curent batch (batch 0) but also the input and \n",
        "                          #the target tensor - the one returned variable (batch) will \n",
        "                          #also include the other contents and not just the batch number\n",
        "            UnderstandDatasetTakeEtc = False \n",
        "            \n",
        "epoch=0\n",
        "for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "     if (epoch==0) and (batch % 125 == 0):\n",
        "        print ('Epoch is = {}\\n Steps per epoch is = {}\\n Batch is = {}\\n InputShape = {}\\n TargShape = {}\\n'\n",
        "                                                     .format(epoch,\n",
        "                                                             steps_per_epoch,\n",
        "                                                             batch,\n",
        "                                                             inp.get_shape().as_list(),\n",
        "                                                             targ.get_shape().as_list()))\n",
        "        "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24000\n",
            "375\n",
            "vocab_inp_size = 9414 \n",
            "vocab_tar_size = 4935 \n",
            "(0, (<tf.Tensor: shape=(64, 16), dtype=int32, numpy=\n",
            "array([[  1,   6,  52, ...,   0,   0,   0],\n",
            "       [  1,  13, 225, ...,   0,   0,   0],\n",
            "       [  1,  54,  63, ...,   0,   0,   0],\n",
            "       ...,\n",
            "       [  1,   6, 507, ...,   0,   0,   0],\n",
            "       [  1, 281,  19, ...,   0,   0,   0],\n",
            "       [  1,   6,  38, ...,   0,   0,   0]], dtype=int32)>, <tf.Tensor: shape=(64, 11), dtype=int32, numpy=\n",
            "array([[   1,   24,    6,  715,    7,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,  396,    8, 1161,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,   28,  133,   81,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,  102,   17,  152,   37,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,    4,  108, 1097,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,    4,   95,   10,   49,   78,    3,    2,    0,    0,    0],\n",
            "       [   1,   27,   11,   21,  418,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   22,    6,   47,   10,    7,    2,    0,    0,    0,    0],\n",
            "       [   1,   14,   87,   12,   64,  503,    3,    2,    0,    0,    0],\n",
            "       [   1,   32,    8,   14,   50,   15,   58,    7,    2,    0,    0],\n",
            "       [   1,   68, 2962,   92,  315,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   52,  169,    4,   43,    7,    2,    0,    0,    0,    0],\n",
            "       [   1,    8,   19, 2711,    7,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,  765,  313,  224,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,    5,  270,    9,  701, 1140,    3,    2,    0,    0,    0],\n",
            "       [   1,   10,   11,    9,  231,  381,    3,    2,    0,    0,    0],\n",
            "       [   1,    4,  538,    6,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,   31,   97,   75,   53,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   56,   22,   20,  131,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,    4,  118,   15,   13, 1274,    3,    2,    0,    0,    0],\n",
            "       [   1,    4,  123,   36,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,    5,  629,   35,    9,  620,    3,    2,    0,    0,    0],\n",
            "       [   1,    4,   30,   12,   63,   10,    3,    2,    0,    0,    0],\n",
            "       [   1,   39,   28,   53,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,    5,   26,  101,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,    4,  251,    6,   15,   53,    3,    2,    0,    0,    0],\n",
            "       [   1,   14,   11,   34,   89,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   28,   42,   34, 2018,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,    4,  230,   82,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,    4,   18,  336,  240,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   84,   59,  126,    8,  330,    3,    2,    0,    0,    0],\n",
            "       [   1,    4,   35,   20,  352,  159,    3,    2,    0,    0,    0],\n",
            "       [   1,    6,  663,    3,    2,    0,    0,    0,    0,    0,    0],\n",
            "       [   1,  157,    8, 1151,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,   26,    4,  334,   15,    6,    7,    2,    0,    0,    0],\n",
            "       [   1,    6,   29,    9,  177,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   71,    8,   21,   83,    7,    2,    0,    0,    0,    0],\n",
            "       [   1,   19,    8,   48, 1378,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,    4,   38,   77,    6,  949,    3,    2,    0,    0,    0],\n",
            "       [   1,   82,    8,  528,  585,    7,    2,    0,    0,    0,    0],\n",
            "       [   1,  428,   24,  428,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,    4,  212,  111,   21,  407,    3,    2,    0,    0,    0],\n",
            "       [   1,   66,   84,    8,   89,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   14,   11,   67,  107,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,  267,   12,    6,  220,    7,    2,    0,    0,    0,    0],\n",
            "       [   1,   14,   70,   76,   89,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   10,   11,  764,  197,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   16,   23,  645,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,  237,  251,   84,    3,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,   36,  163,  569,   50,   45,    3,    2,    0,    0,    0],\n",
            "       [   1,    4,   18,  209,    9, 2340,    3,    2,    0,    0,    0],\n",
            "       [   1,   16,   23,   67,  854,    3,    2,    0,    0,    0,    0],\n",
            "       [   1,   52,  138,    8,   31,  768,    7,    2,    0,    0,    0],\n",
            "       [   1,   96,    4,   36,    7,    2,    0,    0,    0,    0,    0],\n",
            "       [   1,  844,   37,    2,    0,    0,    0,    0,    0,    0,    0],\n",
            "       [   1,    5,  173,   17,    9,  993,    3,    2,    0,    0,    0],\n",
            "       [   1,   22,    6,  464,  491,    7,    2,    0,    0,    0,    0],\n",
            "       [   1,   25,    4, 1050,    9,  471,    7,    2,    0,    0,    0],\n",
            "       [   1,   52, 1338,   37,    2,    0,    0,    0,    0,    0,    0],\n",
            "       [   1,    4,  143,   33,  158,   49,   78,    3,    2,    0,    0],\n",
            "       [   1,    4,   26,   55,   13, 2375,    3,    2,    0,    0,    0],\n",
            "       [   1,   52,  447,   22,    6,   47,    7,    2,    0,    0,    0],\n",
            "       [   1,    4,   65,  105,   21,  735,    3,    2,    0,    0,    0],\n",
            "       [   1,   25,    4,  127,   15,    5,    7,    2,    0,    0,    0]],\n",
            "      dtype=int32)>))\n",
            "Epoch is = 0\n",
            " Steps per epoch is = 375\n",
            " Batch is = 0\n",
            " InputShape = [64, 16]\n",
            " TargShape = [64, 11]\n",
            "\n",
            "Epoch is = 0\n",
            " Steps per epoch is = 375\n",
            " Batch is = 125\n",
            " InputShape = [64, 16]\n",
            " TargShape = [64, 11]\n",
            "\n",
            "Epoch is = 0\n",
            " Steps per epoch is = 375\n",
            " Batch is = 250\n",
            " InputShape = [64, 16]\n",
            " TargShape = [64, 11]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc6-NK1GtWQt",
        "outputId": "13665d69-0d27-4ad3-fa8f-bb1dc4d11e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## Write the encoder and decoder model\n",
        "\n",
        "Implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from [Luong's paper](https://arxiv.org/abs/1508.04025v5). \n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
        "\n",
        "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n",
        "\n",
        "Here are the equations that are implemented:\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "This tutorial uses [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf) for the encoder. Let's decide on notation before writing the simplified form:\n",
        "\n",
        "* FC = Fully connected (dense) layer\n",
        "* EO = Encoder output\n",
        "* H = hidden state\n",
        "* X = input to the decoder\n",
        "\n",
        "And the pseudo-code:\n",
        "\n",
        "* `score = FC(tanh(FC(EO) + FC(H)))`\n",
        "* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
        "* `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
        "* `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
        "* `merged vector = concat(embedding output, context vector)`\n",
        "* This merged vector is then given to the GRU\n",
        "\n",
        "The shapes of all the vectors at each step have been specified in the comments in the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    print (\"self.batch_sz = {}, self.enc_units = {}, \\\n",
        "            self.embedding.input_dim = {}, self.embedding.output_dim = {},\\\n",
        "            vocab_size = {}, embedding_dim = {}\".format(self.batch_sz, \\\n",
        "                                                self.enc_units, \\\n",
        "                                                self.embedding.input_dim,\\\n",
        "                                                self.embedding.output_dim,\\\n",
        "                                                vocab_size,\n",
        "                                                embedding_dim))\n",
        "                                \n",
        "  def call(self, x, hidden):\n",
        "    # print (x.shape)\n",
        "    x = self.embedding(x)\n",
        "    # print (x.shape)\n",
        "    # print (\"hidden shape is = {}\". format(hidden.shape))\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gSVh05Jl6l",
        "outputId": "f7456182-52e9-477b-a6c1-c74d027f04cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "#print (sample_hidden.shape)\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "self.batch_sz = 64, self.enc_units = 1024,             self.embedding.input_dim = 9414, self.embedding.output_dim = 256,            vocab_size = 9414, embedding_dim = 256\n",
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umohpBN2OM94"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    # print (\"in call function\")\n",
        "    # print (query.shape)\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    # print (query_with_time_axis.shape)\n",
        "    # print (values.shape)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "    # print (score.shape)\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k534zTHiDjQU",
        "outputId": "bdc1d949-8b97-4ffc-e12d-8c17632cc164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp",
        "outputId": "ae458397-ca1a-4012-8b5d-cf895bf89e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF",
        "outputId": "781c592c-ab73-4fef-e50e-8be1c84a1801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "checkpoint_dir = '/content/gdrive'\n",
        "drive.mount (checkpoint_dir, force_remount=True)\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'My Drive/Training/ckpt_nmt')\n",
        "print (checkpoint_prefix)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                  encoder=encoder,\n",
        "                                  decoder=decoder)\n",
        "\n",
        "# checkpoint_dir = './training_checkpoints'\n",
        "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "# checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "#                                  encoder=encoder,\n",
        "#                                  decoder=decoder)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Training/ckpt_nmt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp8mhy4TnRbI"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.flush_and_unmount()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "# https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3\n",
        "# https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/\n",
        "# https://stats.stackexchange.com/questions/241985/understanding-lstm-units-vs-cells\n",
        "# https://stats.stackexchange.com/questions/179101/structure-of-recurrent-neural-network-lstm-gru/275518#275518\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihxq1B1nmKkC",
        "outputId": "11897b23-d9d6-474e-ae76-116bf2320363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 1\n",
        "print_msg = True\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  print (enc_hidden)\n",
        "  total_loss = 0\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    if ((epoch == 0) and (batch % 125 == 0) and (print_msg)):\n",
        "        x, y = (inp, targ)\n",
        "        print (\"*************************************\\n\")\n",
        "        print ('Epoch is = {}\\n Steps per epoch is = {}\\n Batch is = {}\\n InputShape = {}\\n TargShape = {}\\n  Input batch size = {}\\n \\\n",
        "                Target batch size = {} \\n Input length is = {} \\n Target length is = {} \\n Input is = {} \\n Target is = {} \\n'\n",
        "                                                     .format(epoch,\n",
        "                                                             steps_per_epoch,\n",
        "                                                             batch,\n",
        "                                                             inp.get_shape().as_list(),\n",
        "                                                             targ.get_shape().as_list(),\n",
        "                                                             len(x),\n",
        "                                                             len(y),\n",
        "                                                             len(x[0]),\n",
        "                                                             len(y[0]),\n",
        "                                                             x[0],                                                             \n",
        "                                                             y[0]))\n",
        "    \n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "    \n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  # if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]], shape=(64, 1024), dtype=float32)\n",
            "*************************************\n",
            "\n",
            "Epoch is = 0\n",
            " Steps per epoch is = 375\n",
            " Batch is = 0\n",
            " InputShape = [64, 16]\n",
            " TargShape = [64, 11]\n",
            "  Input batch size = 64\n",
            "                 Target batch size = 64 \n",
            " Input length is = 16 \n",
            " Target length is = 11 \n",
            " Input is = [   1  187   14 4778   15  844    3    2    0    0    0    0    0    0\n",
            "    0    0] \n",
            " Target is = [  1   4  70 456  21 837   3   2   0   0   0] \n",
            "\n",
            "Epoch 1 Batch 0 Loss 1.1121\n",
            "Epoch 1 Batch 100 Loss 1.1368\n",
            "*************************************\n",
            "\n",
            "Epoch is = 0\n",
            " Steps per epoch is = 375\n",
            " Batch is = 125\n",
            " InputShape = [64, 16]\n",
            " TargShape = [64, 11]\n",
            "  Input batch size = 64\n",
            "                 Target batch size = 64 \n",
            " Input length is = 16 \n",
            " Target length is = 11 \n",
            " Input is = [   1    4   12 1351   15  922    3    2    0    0    0    0    0    0\n",
            "    0    0] \n",
            " Target is = [  1   5 696  17   9 993   3   2   0   0   0] \n",
            "\n",
            "Epoch 1 Batch 200 Loss 1.0551\n",
            "*************************************\n",
            "\n",
            "Epoch is = 0\n",
            " Steps per epoch is = 375\n",
            " Batch is = 250\n",
            " InputShape = [64, 16]\n",
            " TargShape = [64, 11]\n",
            "  Input batch size = 64\n",
            "                 Target batch size = 64 \n",
            " Input length is = 16 \n",
            " Target length is = 11 \n",
            " Input is = [   1 1791 3321   14  102    3    2    0    0    0    0    0    0    0\n",
            "    0    0] \n",
            " Target is = [   1    4   62 3167   54   97    3    2    0    0    0] \n",
            "\n",
            "Epoch 1 Batch 300 Loss 0.9319\n",
            "Epoch 1 Loss 0.9898\n",
            "Time taken for 1 epoch 2746.619951725006 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PpVcHAO1ARe"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5hQWlbN3jGF"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9zUHzg3jGI"
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpT9D5_OgP6",
        "outputId": "9152d1db-7b36-4dda-c375-432ef3e56a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f62a7e4e7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrAM0FDomq3E",
        "outputId": "23371fb0-c92d-47ad-db24-8b77a4f93fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: it s cold very much . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSx2iM36EZQZ",
        "outputId": "c52310f2-96b1-48c7-9c7f-1e9f0792fbc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        }
      },
      "source": [
        "translate(u'esta es mi vida.')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> esta es mi vida . <end>\n",
            "Predicted translation: this is my friend . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3LLCx3ZE0Ls"
      },
      "source": [
        "translate(u'¿todavia estan en casa?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUQVLVqUE1YW",
        "outputId": "daae5470-767c-479d-d9ec-d4080efe7f57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        }
      },
      "source": [
        "# wrong translation\n",
        "translate(u'trata de averiguarlo.')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> trata de averiguarlo . <end>\n",
            "Predicted translation: we ll be confident . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTe5P5ioMJwN"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
        "* Experiment with training on a larger dataset, or using more epochs\n"
      ]
    }
  ]
}